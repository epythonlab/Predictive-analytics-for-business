{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business and Data Unerstanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What decisions need to be made?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the prediction made by data analyst, the manager need to be made that the new loan applicants are creditworthy or non-creditworthy and approve them if the applicants are creditworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What data is needed to inform those decisions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I have selected 12 (6 categorical and 6 numerical) variables as predictor variables and `Credit-Application-Result` as a target variable to inform those decisions. I have listed the variables that are useful for our prediction below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Account-Balance\n",
    "2. Duration-of-Credit-Month\n",
    "3. Payment-Status-of-Previous-Credit\n",
    "4. Credit-Amount5-Purpose\n",
    "5. Purpose\n",
    "6. Value-Savings-Stocks\n",
    "7. Length-of-current-employment\n",
    "8. Instalment-per-cent\n",
    "9. Most-valuable-available-asset\n",
    "10. Age-years\n",
    "11. Type-of-apartment\n",
    "12. No-of-Credits-at-this-Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What kind of model (Continuous, Binary, Non-Binary, Time-Series) do we need to use to help make these decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a binary model in which there are two classes that going to be classified as creditworthy(approved) or non-creditworthy(not approved)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Building the Training Set\n",
    "\n",
    "Build your training set given the data provided to you. The data has been cleaned up for you already so you shouldn’t need to convert any data fields to the appropriate data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer this question:\n",
    "In your cleanup process, which fields did you remove or impute? Please justify why you removed or imputed these fields. Visualizations are encouraged.\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I have done a lot of processes to cleanup and format the dataset.\n",
    "\n",
    "**Step 1. Dealing with missing data:**\n",
    "\n",
    " As we can see in the following table there are two variables that has missing values. The `Duration-in-Current-address` has missing 344(68.8%) and the `Age-years` has missing 12(2.4%) out of 500 records. The `Duration-in-Current-address` variable has missing almost 69 percent of our data and probably our model will not have a significant changes if we drop this variable. Therefore, I have removed the `Duration-in-Current-address` variable because it is no longer needed for our prediction. But we impute the `Age-years` variable by comparing mean and median values of this variable.\n",
    "\n",
    "![img](images/missing.png)\n",
    "\n",
    "The mean and meadian values of the `Age-years` variable are **35.64** and **33** respectively. Therefore, I have imputed the `Age-years` variable with the meadian value because the mean value will have an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Identify low variablity:**\n",
    "The variables distributions are shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above distribution plot, we can see that the `Guarantors`, `No-of-dependents`, and `Foreign-Worker` variables are skewed the majority of the data points to one category and `Concurrent-Credits` and  `Occupation` variables have one data point. These variables are low variability and no longer needed for our prediction. `Telephone` also not important because it has two unique values. Therefore, I have decided to remove these variables from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Ploting the correlation of numerical variables** \n",
    "![png](images/heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the above correlation graph shows that there are no variables having strongly correlated(high correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the dataset, I have found 13 columns(12 predictive variables and a target variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train your classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which predictor variables are significant or the most important? Please show the p-values or variable importance charts for all of your predictor variables.\n",
    "\n",
    "2. Validate your model against the Validation set. What was the overall percent accuracy? Show the confusion matrix. Are there any bias seen in the model’s predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have splitted the dataset into training(70%) and testing(30%) set with random state of 1. Then, I have trained and validated the test dataset using the following classification models:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. RandomForestClassifier\n",
    "3. DecisionTreeClassifier\n",
    "4. GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](images/lr_importance.png)\n",
    "As per the above feature importance chart, the following predictor variables are most significant where they have higher values(which mean that the p-value < 0.05)\n",
    "- Payment-Status-of-Previous Credit_Some Problems\n",
    "-  Credit-Amount\n",
    "- Duration-of-Credit-Month\n",
    "- Account-Balance_Some Balance\n",
    "- Purpose\n",
    "- Lenght-of-current-employment_<1yr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is formed from the four outcomes produced as a result of binary classification.\n",
    "- True Positive(TP)\n",
    "- True Negative(TN)\n",
    "- False Positive(FP)\n",
    "- False Negative(FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](images/lr_mat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, I have calculated the accuracy of `creditworty` and `Non-Creditworthy`.\n",
    "\n",
    "$$Credityworthy = TP / (TP + FP) = 93 / (93 + 10) * 100 = 90% $$\n",
    "$$Non-Creditworthy = TN / (TN + FN) = 22 / (22 + 23) * 100 = 47% $$\n",
    "$$Overall-accuracy = TN + TP (TN + TP + FP + FN) * 100 = 77% $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy of the logistic regression model against the validation set is 77%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the confusion matrix, the model is slightly biased towards predicting Non-Creditworthy which mean that 100 - 47 = 53% are incorrectely predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Account-Balance_Some Balance`, `Duration-of-Credit-Month`, and `Credit-Amount` are the most significant variables that are used to classify either Creditworthy or Non-Creditworthy using decision tree classifier model. ![img](images/dtc_features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/dt_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above confusion matrix of Decision Tree model, the accuracy of each classes are:\n",
    "\n",
    "- Credityworthy = TP / (TP + FP) = 90 / (90 + 13) * 100 = 84%\n",
    "\n",
    "- Non-Creditworthy = TN / (TN + FN) = 16 / (16 + 31) * 100 = 34%\n",
    "\n",
    "- Overall-accuracy = (TP + TN) / (TP + TN + FN + FP) = (90 + 16) / (90 + 16 + 31 + 10) = 71%\n",
    "\n",
    "Therefore, the model is biased towards predicting Crediworthy and the model is incorrectly predicting 66% of Non-Creditworthy. In the validation data set, the model achieved accuracy of 71%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using RandomForest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In random forest model, there are 500 trees with 8 variables and each variables are tested at each split. The most significant features were identified and plotted in the following bar plot.\n",
    "![img](images/rf_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used some tunning parameters to improve the performane of the model and the model is not biased towards predicting Non-creditworthy. \n",
    "- Credityworthy = TP / (TP + FP) = 92 / (92 + 11) * 100 = 84%\n",
    "\n",
    "- Non-Creditworthy = TN / (TN + FN) = 26 / (26 + 21) * 100 = 55%\n",
    "\n",
    "- Overall-accuracy = (TP + TN) / (TP + TN + FN + FP) = (92 + 26) / (92 + 26+ 21 + 11) = 79%\n",
    "\n",
    "Therefore, from the confusion matrix the model predicting 84% as `Creditworthy` while 55% is predicting as `Non-Creditworthy`. The overall accuracy of the model is 79%.\n",
    "\n",
    "![img](images/rf_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Boosted Model(GradientBoosting Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this model, I have got the following most significant variables as best predictors.\n",
    "\n",
    "![img](images/gbc_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/gbc_report.png)\n",
    "- Credityworthy = TP / (TP + FP) = 83 / (83 + 20) * 100 = 81%\n",
    "\n",
    "- Non-Creditworthy = TN / (TN + FN) = 24 / (24 + 23) * 100 = 51%\n",
    "\n",
    "- Overall-accuracy = (TP + TN) / (TP + TN + FN + FP) = (83 + 24) / (83 + 24+ 23 + 20) = 71%\n",
    "\n",
    "Accourdingly the confusion matrix and the classification report, the model is slightly biased towards predicting Creditworthy and 49% is incorrectly predicted. The accuracy of Creditworthy is 81% while Non-Creditworthy is 51% and the percentage accuracy is 71."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Writeup\n",
    "\n",
    "Compare all of the models’ performance against each other. Decide on the best model and score your new customers.\n",
    "\n",
    "***Important:*** Your manager only cares about how accurate you can identify people who qualify and do not qualify for loans for this problem.\n",
    "\n",
    "Write a brief report on how you came up with your classification model and write down how many of the new customers would qualify for a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To came up with the best classification model, I have gone through the following steps:\n",
    "\n",
    "1. cleansing, formating, and blending the dataset toghter\n",
    "2. Dealing with missing data\n",
    "3. Identifying variables with the low varibility and drop them\n",
    "4. Identifying the best predictors\n",
    "5. Spliting the data into training and validation set based on the criteria given(70% training, 30% test/validation dataset)\n",
    "6. train the classification models with tunning parameters to improve the model performance\n",
    "7. Evaluate the model performance using `Precision, recall, and f1-score`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/mc.png)\n",
    "The higher the **AUC**, the better the performance of the model at distinguishing between the positive and negative classes. So, the higher the AUC value for a classifier, the better its ability to distinguish between positive and negative classes. The higher the overall accuracy, the better the performance of the model.\n",
    "\n",
    "Therefore, from the above **ROC CURVE GRAPH and Model Comparison Table**, the best performed model is **Random Forest Model**. The random forest model accieved 80% of AUC and 79% of Overall Accuracy as compared to other three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, based on the best performed model(using Random Forest Classifier) that`418` of individual loans classified as **Creditworthy** and `82` of individual loans as **Non-Creditworthy** from the new customer score data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that I have used different paramaters to improve the model performance and evalauted the model using Precision, Recall, and F1-score. I would like to suggest that we can have a chance to improve the model performance and get the best accurated model using different tunning parameters and cross validations mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "*Copyright &copy; 2021 <a href=\"https://youtube.com/c/epythonlab/\">EPYTHON LAB</a>.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
